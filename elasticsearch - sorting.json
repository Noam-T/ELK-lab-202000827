// sorting our data
// URI BASE
#curl -X GET "localhost:9200/movies/_search?pretty&sort=year"

GET my_kibana_sample_data_flights/_search
{
	"query": {
		"bool": {
			"must": [
				{
					"term": {
						"DestCountry": {
							"value": "DE"
						}
					}
				}
			]
		}
	},
	"sort": [
		{
			"dayOfWeek": {
				"order": "desc"
			}
		}
	]
}

// JSON BASE
#curl -X GET "localhost:9200/movies/_search?pretty" -d'
#
#{
#	"from" : 2,
#	"size" : 2,
#	"query" : {
#		"match": {"genre": "Sci-Fi"}
#	},
#	"sort" : "year"
#}
#
#'

GET my_kibana_sample_data_flights/_search
{
	"query": {
		"match": {
			"Carrier": "JetBeats"
		}
	},
	"sort": [
		{
			"dayOfWeek": {
				"order": "desc"
			}
		}
	]
}



// Sort by a title - But our title is using analyzer which mean we cant sort it
// Solution is to crete in our mapping a keyword field instead. 

#curl -X GET "localhost:9200/movies/_search?pretty" -d'

#{
#	"from" : 2,
#	"size" : 2,
#	"query" : {
#		"match": {"genre": "Sci-Fi"}
#	},
#	"sort" : "title"
#}

#'  // >> Will break with an error due to it being a text field 

GET my_kibana_sample_data_flights/_search
{
	"from" : 2,
	"size" : 2,
	"query" : {
		"match": {"Origin": "Frankfurt am Main Airport"}
	},
	"sort" : "Dest"
}

// the Solution is the following mapping (we need to do it beforehand 
//	as we cant change it on running)
1. Delete the Index -  curl -XDELETE localhost:9200/movies
2. Create the new one

PUT my_kibana_sample_data_flights
{
  "mappings": {
    "properties": {
      "FlightNum":{
        "type": "text"
      },
      "DistanceMiles":{
        "type": "float"
      },
      "Dest":{
        "type": "text",
        "analyzer": "english"
         "fields": {
            "raw": {
              "type": "keyword"
            }      }
    }
  },
    "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete": {
          "tokenizer": "autocomplete",
          "filter": [
            "lowercase"
          ]
        },
        "autocomplete_search": {
          "tokenizer": "lowercase"
        }
      },
      "tokenizer": {
        "autocomplete": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  }
}




